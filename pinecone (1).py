# -*- coding: utf-8 -*-
"""pinecone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CdKeokEDzNU5i1hYU0yc7-XGwjEF4Fp-
"""

# requirements.txt içeriğini oluşturma
requirements = """
langchain-openai
langchain-pinecone
langchain-community
langchain-text-splitters
"""

# requirements.txt dosyasını kaydetme
with open('requirements.txt', 'w') as f:
    f.write(requirements)

pip install python-docx

!pip install unstructured

!pip install "unstructured[docx]"

!pip install -r requirements.txt

from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

import os
import glob
import nltk
nltk.download('punkt')

!mkdir data

m_loader = DirectoryLoader(
    path="data",
    glob="**/*.txt"
)
docs = m_loader.load()

docs[0]

import os

# Tanımlamaları yapın (örnek olarak)
PINECONE_API_KEY = "88c6a8f6-fc21-4b1d-bb91-3951f3fb16f8"
OPENAI_API_KEY = "sk-proj-EhlifDQnRaJU2OWVa8rnT3BlbkFJCPYZxGEBVyZa6luzOX46"
index_name = "allechatbot"

# Ortam değişkenlerini ayarlayın
os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY
os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY

# OpenAIEmbeddings sınıfını kullanarak modeli başlatın
from langchain_openai import OpenAIEmbeddings

m_embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
    )

#split the document into chunks
m_splitter = RecursiveCharacterTextSplitter()
chunks = m_splitter.split_documents(docs)

chunks[0]

m_vectorstore = PineconeVectorStore.from_documents(chunks, m_embeddings, index_name="allechatbot")

"""# Searching Pinecone"""

from pinecone import Pinecone
import os

class PineconeClientSingleton:
    _instance = None

    @staticmethod
    def get_instance():
        if PineconeClientSingleton._instance is None:
            # Initialize Pinecone client
            api_key = os.getenv("PINECONE_API_KEY")
            PineconeClientSingleton._instance = Pinecone(api_key=api_key)
            return PineconeClientSingleton._instance

    @staticmethod
    def create_index(name, dimension, metric, spec):
        if name not in PineconeClientSingleton._instance.list_indexes().names():
            PineconeClientSingleton._instance.create_index(
                name=name,
                dimension=dimension,
                metric=metric,
                spec=spec
            )

    @staticmethod
    def get_index(name):
        if name not in PineconeClientSingleton._instance.list_indexes().names():
            False
        else:
            return PineconeClientSingleton._instance.Index(name)

    @staticmethod
    def delete_index(name):
        if name in PineconeClientSingleton._instance.list_indexes().names():
            PineconeClientSingleton._instance.delete_index(name)


# Create the Pinecone client
pinecone_client = PineconeClientSingleton.get_instance()
index = PineconeClientSingleton.get_index("allechatbot")

vectorstore_client = PineconeVectorStore(index=index, embedding=m_embeddings)

query = "Tercüme Hizmetlerinizden bahsedebilir misiniz?"
results = vectorstore_client.similarity_search(query)
results

from langchain_openai import ChatOpenAI
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.prompts import PromptTemplate

# Initialize the language model
m_llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0
)

retriever = vectorstore_client.as_retriever()
m_qa = RetrievalQA.from_chain_type(
    llm=m_llm,
    chain_type="stuff",
    retriever=retriever
)

# Define the prompt template
prompt_template = """
Given the context of the following documents:
{context}

Please answer the user's query succinctly in no more than 100 words.

Query: {query}
"""

prompt = PromptTemplate(
    input_variables=["context", "query"],
    template=prompt_template
)

input_data = {
    "context": """

    """,
    "query":
        "Tercüme Hizmetlerinizden bahsedebilir misiniz?"
}

formatted_prompt = prompt.format(**input_data)
result = m_qa.invoke({"query": formatted_prompt})
print(result)

input_data = {
    "context": """

    """,
    "query":
        "Peki ucretleriniz ne sekildedir? Ucretleriniz hakkinda bilgi verebilir misiniz?"
}

formatted_prompt = prompt.format(**input_data)
result = m_qa.invoke({"query": formatted_prompt})
print(result)

input_data = {
    "context": """

    """,
    "query":
        "Sirketinizin kac subesi var"
}

formatted_prompt = prompt.format(**input_data)
result = m_qa.invoke({"query": formatted_prompt})
print(result)